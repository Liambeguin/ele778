\documentclass[11pt]{article}
\input{ets_page.tex}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage[
	pdftitle   ={ELE778 rapport de laboratoire 2.2},
	pdfsubject ={Detail de l'implementation d'un reseau de Neurones a
		retro-propagation du gradient d'erreur},
	pdfauthor  ={Liam BEGUIN - Louis LAPORTE},
	pdfkeywords={Neural network, mini-batch SGD, TiDigits classification},
	citecolor  ={red},
	filecolor  ={},
	urlcolor   =blue,
]{hyperref} % Required for links
\usepackage{enumitem}



\EtsPageCourse{ELE778-01}
	{Intelligence artificielle: r\'eseaux neuroniques et syst\`emes experts}
\EtsPageTitle{Laboratoire 2}
\EtsPageProf{Cynthia}{Moussa}
\EtsPageAuthA{Liam}{Beguin}{BEGL02129304}
\EtsPageAuthB{Louis}{Laporte}{LAPL14128903}
\EtsPageAuthC{}{}{}

\begin{document}
\EtsPageGenerate
\tableofcontents
% \newpage

\section{Notations et symboles}
\begin{tabular}{p{1.75cm}p{10cm}}
	${\bf x}$ & Entr\'ee du r\'eseau, \\
	${\bf y_x}$ & Sortie attendue du r\'eseau pour l'entr\'ee {\bf x}, \\
	${\bf \hat y_x}$ & Pr\'ediction du r\'eseau pour l'entr\'ee {\bf x}, \\
	${\bf a} \circ {\bf b}$ & Produit matriciel de
		\href{https://en.wikipedia.org/wiki/Hadamard_product_(matrices)}
		{Hadamard}, \\
	$\epsilon_{\bf x}$ & taux d'erreur lorsque le reseau evalue l'entree ${\bf x}$, \\
	$\sigma(z)$ & Fonction d'activation generique, \\
	$\sigma'(z)$ & Deriv\'ee de la fonction d'activation, \\
	$C(w, b)$ & Fonction de cout g\'en\'erique, \\
	${\bf C}'(w, b)$ & D\'eriv\'ee de la fonction de cout g\'en\'erique par
		rapport a ${\bf \hat y}$, \\
	$\nabla_bC$ & \href{https://en.wikipedia.org/wiki/Matrix_calculus}
		{D\'eriv\'ees de $C$} par rapport aux seuils de la couche $(l)$, \\
	$\nabla_wC$ & D\'eriv\'ees de $C$ par rapport aux poids de la couche $(l)$, \\
	$d^{(l)}$ & Nombre d'elements sur la couche $(l)$, \\
	${\bf w}^{(l)}(T)$ & Ensemble des poids de la couche $(l)$ \`a l'\'epoque $T$, \\
\end{tabular}
\newpage



\section{Introduction}
Après avoir fait une étape de pré-traitement lors du laboratoire 2.1, nous
allons maintenant implémenter un reseau de neurones a retro-propagation du
gradient d’erreur dans le but de classifier, le plus précisément possible,
le dataset TiDigits (english spoken digits from 1 to 9).

Pour implémenter ce reseau de neurones, nous utiliserons Python.
Voici une liste d'avantages versus inconvenients qui ont motive notre choix
pour ce language :
\begin{itemize}
	\item avantages:
		\subitem rapidite de developpement
		\subitem lang interprete, pas de compilation
		\subitem modules de calcul matriciel performants
		\subitem language accessible
	\item inconvenients:
		\subitem lang interprete donc moins rapide \\
\end{itemize}

Dans un premier temps, nous entrainerons le reseau sur un set de donnees puis
nous evaluerons sa capacite a generaliser sur de nouvelles donnees non utilisees
lors de l'entrainement.

\section{Implementation du r\'eseau de neurones}
\subsection{Architecture globale}
\begin{figure}[htp]
	\centering
	\includegraphics[scale=.5]{img/neuron.png}
	\caption{Repr\'esentation symbolique d'un neurone.}
\end{figure} \\

Comme dit en introduction, Nous allons impl\'ementer un r\'eseau de neurones a
r\'etro-propagation du gradient d'erreur. Ici, nous \'enoncerons les notations et
\'equations g\'en\'eriques utilis\'ees au sein du r\'eseau.

La majorit\'e des calculs sont effectues sous forme matricielle afin de simplifier
et g\'en\'eraliser les \'equations mais aussi de mieux tirer avantage des ressources
de calcul disponibles.

Etant donne que le r\'eseau doit \^etre tr\`es flexible quant au nombre de
couches et au nombre de neurones, nous utiliserons un tuple (une liste
invariante) pour instancier le r\'eseau.
Chaque \'el\'ement de la liste repr\'esente une couche du r\'eseau
(incluant la couche d'entr\'ee) et sa valeur d\'etermine le nombre de neurones
present sur la couche.
Pour la couche de sortie, nous utiliserons un vecteur de
type \emph{one-hot} ou un seul element est actif \`a la fois.

D'autre part, on nous demande aussi que le reseau soit capable d'utiliser
differentes fonctions d'activation. Base sur cette contrainte, nous avons
choisi d'aussi donner a l'utilisateur la possibilite de choisir entre
differentes fonctions de cout et differentes fonctions de regularization.

\paragraph{Notations: }Soit, $w_{jk}^{(l)}$ le poid connectant le neurone $k$
de la couche $(l-1)$ au neurone $j$ de la couche $(l)$, o\`u :
$$
\left \{
	\begin{aligned}
		&1 \le l \le L         &\qquad \text{layers} \\
		&0 \le k \le d^{(l-1)} &\qquad \text{inputs} \\
		&1 \le j \le d^{(l)}   &\qquad \text{outputs}\\
	\end{aligned}
\right .
$$
On definit donc ${\bf w}^{(l)}$ la matrice de dimension $\Big((l) \times (l-1)\Big)$
connectant la couche $(l-1)$ \`a la couche $(l)$ cr\'eant ainsi, $(L-1)$ matrices
de poids sur l'ensemble du reseau. \\
\paragraph{Feedforward:} cette etape propage l'entree a travers l'ensemble du
reseau et enregistre toutes les variables intermediaires.
\begin{equation}
	\begin{aligned}
		z_j^{(l)} &= \sum_k^{d^{(l-1)}}{w_{jk}^{(l)} \cdot a_{k}^{(l-1)} + b_{j}^{(l)}\\
		{\bf z}_x^{(l)} &= {\bf w}^{(l)} \cdot {\bf a}_x^{(l-1)} + {\bf b}^{(l)}\\
		{\bf a}_x^{(l)} &= \sigma\left({\bf z}_x^{(l)}\right)\\
	\end{aligned}
\end{equation}
Pour avoir la prediction finale du reseau ${\bf \hat{y}}$, il faut repeter l'etape
ci-dessus jusqu'a atteindre la sortie o\`u on obtient:
\begin{equation}
	\begin{aligned}
		{\bf z}_x^{(L)} &= {\bf w}^{(L)} \cdot {\bf a}_x^{(L-1)} + {\bf b}^{(L)}\\
		{\bf \hat{y}}_x &= \sigma\left({\bf z}_x^{(L)}\right)\\
	\end{aligned}
\end{equation}

\paragraph{La fonction de cout}est la fonction utilisee lors du processus
d'entrainement pour evaluer l'erreur entre la prediction du reseau
${\bf \hat{y}}$ et la sortie attendue ${\bf y}$. Les differents fonctions
disponibles seront detaill\'ees dans la section \ref{cost}.
Pour la suite, nous l'appelerons de fa\c con generique: $C({\bf w}, {\bf b})$.

\paragraph{La fonction de regularization}est la fonction utilisee lors du processus
d'entrainement pour p\'enaliser la minimisation de la fonction de cout. Ceci sera
couvert plus en detail dans la section \ref{overfitting}.
Pour la suite, nous l'appelerons de fa\c con generique: $\Omega({\bf w})$.

\paragraph{retro-propagation:} Cette \'etape calcule l'impacte de chaque poids
(et seuil) sur l'erreur finale. Pour obtenir l'erreur $\delta_j^l$ due au neurone
$j$ de la couche $l$, il faut propager l'erreur de la sortie vers l'entree
avec les equations suivantes:

\begin{equation}
	\begin{aligned}
		{\boldsymbol \delta}^{(L)} &= \frac{\partial C}{\partial {\bf z}^{(L)}} =
		\frac{\partial C}{\partial {\bf a}^{(L)}} \circ \frac{\partial {\bf a}^{(L)}}{\partial {\bf z}^{(L)}} \\
		&= \frac{\partial C}{\partial {\bf a}^{(L)}} \circ \sigma'\left({\bf z}^{(L)}\right)
		= \frac{\partial C}{\partial {\bf \hat y}} \circ \sigma'\left({\bf z}^{(L)}\right)\\
		&= C'({\bf w}, {\bf b}) \circ \sigma'\left({\bf z}^{(L)}\right)\\
	\end{aligned}
\end{equation}

De la m\^eme mani\`ere, on definit une notation recursive:
\begin{equation}
	\begin{aligned}
		\delta_j^{(l)} &= \frac{\partial C}{\partial z_j^{(l)}} =
		\sum_k{\frac{\partial C}{\partial z_k^{(l+1)}} \cdot
		\frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}}} \\
		&= \sum_k\delta_k^{(l+1)} \cdot \frac{\partial z_k^{(l+1)}}{\partial z_j^{(l)}} \\
	\end{aligned}
\end{equation}
Or,
$$
	z_k^{(l+1)} &= \sum_j^{d^{(l)}}{w_{jk}^{(l+1)}\cdot a_j^{(l)} + b_k^{(l+1)}}\\
$$
Donc,
$$
	\frac{\partial z_k^{(l+1)}}{\partial z_j^l} =
	w_{kj}^{(l+1)}\cdot \sigma'\left( z_j^{(l)}\right)\\
$$
Et,
$$
	\delta_j^{(l)} = \sigma'\left( z_j^{(l)}\right)\cdot \sum_k\delta_k^{(l+1)} \cdot
	w_{kj}^{(l+1)} \\
$$
Nous pouvons maintenant utiliser $\delta_{j}^{(l)}$ pour determiner
l'impacte de chaque seuil sur la fonction de cout:
\begin{equation}
	\begin{aligned}
		\frac{\partial C}{\partial b_j^{(l)}} &= \frac{\partial C}{\partial z_j^{(l)}}
		\cdot \frac{\partial z_j^{(l)}}{\partial b_j^{(l)}} \\
		&= \delta_j^{(l)}  \\
	\end{aligned}
\end{equation}
Idem pour les poids:
\begin{equation}
	\begin{aligned}
		\frac{\partial C}{\partial w_{jk}^{(l)}} &= \frac{\partial C}{\partial z_j^{(l)}}
		\cdot\frac{\partial z_j^{(l)}}{\partial w_{jk}^{(l)}} = \delta_j^{(l)}
		\cdot \frac{\partial z_j^{(l)}}{\partial w_{jk}^{(l)}} =
		\delta_j^{(l)} \cdot a_{k}^{(l-1)} \\
		&= \delta_j^{(l)} \cdot	\sigma\left(z_{k}^{(l-1)} \right)
	\end{aligned}
\end{equation}
Et sous formes matricielles:
\begin{equation}
	\begin{aligned}
		\boldsymbol \delta^{(l)} &= \left(\left({\bf w}^{(l+1)} \right)^{T}
		\cdot \boldsymbol \delta^{(l+1)} \right)
		\circ \sigma'\left( {\bf z}^{(l)}\right) \\
		\nabla_bC = \frac{\partial C}{\partial {\bf b}^{(l)}} &=
		\boldsymbol\delta^{(l)}  \\
		\nabla_wC = \frac{\partial C}{\partial {\bf w}^{(l)}} &=
		\boldsymbol\delta^{(l)} \cdot \left(\sigma\left({\bf z}^{(l-1)}\right)\right)^{T}\\
	\end{aligned}
\end{equation}

\paragraph{Actualisation des poids:} maintenant que le reseau est en mesure de
quantifier l'erreur due a chaque neurone, nous pouvons mettre a jour les poids
\`a l'aide des expressions suivantes:
\begin{equation}
	\begin{aligned}
		{\bf b}^{(l)}(T+1) &= {\bf b}^{(l)}(T) - \eta \nabla_bC \\
		{\bf w}^{(l)}(T+1) &= {\bf w}^{(l)}(T) - \eta \cdot \bigg( \nabla_bC + \Omega'({\bf w})\bigg)\\
	\end{aligned}
\end{equation}



\subsection{Extraction des donn\'ees}
% TODO
% preprocessing, chx du 40, 50 ,60
% normalisation du dataset
% classification homme / femme


\subsection{Initialisation du res\'eau}
Lorsque le reseau est instanci\'e, les parametres tels que $\eta$ et $\lambda$
sont copi\'es dans l'object
\emph{Network} et les matrices de poids et de seuils (\emph{weights} et
\emph{biases}) sont g\'en\'er\'ees. Les dimensions de ces matrices sont
determin\'ees en fonction du nombre de neurones sur chaque couche et du nombre
de couches $L$.


D'autre part, du fait que la fonction aleatoire utilis\'ee pour l'initialisation
des poids donne une distribution avec les parametres suivants $\mu=0$ et $\sigma=1$,
l'initialisation des poids a tendance a saturer les neurones et ralentire l'apprentissage.
car :
\begin{equation}
	\begin{aligned}
		Var(z) &= \sum_{i=1}^N(Var(x_i)) \\
		Var(z) &= N \cdot Var(x) \\
		\sigma(z) &= \sqrt{N} \cdot 1 \\
		\sigma(z) &= \sqrt{N}
	\end{aligned}
\end{equation}




\begin{figure}[htp]
	\centering
	\includegraphics[scale=.5]{img/sigmoid_sat.png}
	\caption{Visualisation de la saturation de la fonction \emph{sigmoid} si
	$|z| \ge 5$.}
\end{figure} \\

Pour contrer cette saturation et donc accelerer les premi\`eres \'epoques
d'apprentissage, nous utiliserons une distribution plus \'etroite avec les
parametres $\mu=0$ et $\sigma =1/\sqrt{N}$ \\


\lstset{tabsize = 4,
frame=lines,
numbers=left,
captionpos=b,
caption = {Initialisation des poids et seuils},
language = python,
basicstyle=\small} \\
\begin{lstlisting}
self.biases  = [ np.random.randn(y, 1) for y in struct[1:] ]
self.weights = [ np.random.randn(y, x) / np.sqrt(x) \
				for x, y in zip(struct[:-1], struct[1:]) ]
\end{lstlisting}


\subsection{Fonctions d'activation}
% sigmoid  -> probability dist
% tanh     -> soft threshold (linear arround 0 hard threshold far)
% softplus -> linear above 0
% add plots



\subsection{Fonctions de cout}\label{cost}
% cross-entropy -> prevent neuron saturation
Comme ennonce precedemment, nous offrons aussi \`a l'utilisateur la possibilite
de choisir parmis differentes fonctions de couts, la fonctions que le reseau
cherche a minimiser en ajustant les poids et seuils des connections.
\paragraph{quadratic} est une
\href{https://fr.wikipedia.org/wiki/Erreur_quadratique_moyenne}
{fonction d'erreur quadratique moyenne}. Cete fonction est la plus simple et
est definie de la maniere suivante:
\begin{equation}
	\begin{aligned}
		C(w, b) &= \frac{1}{2\cdot d^{(L)}}\sum_{j}^{d^{(L)}}{({\bf\hat y} - {\bf y})^2}  \\
		{\bf C'}(w, b) &= \frac{1}{d^{(L)}}\cdot({\bf\hat y} - {\bf y})
	\end{aligned}
\end{equation}
\paragraph{cross-entropy} est la seconde option et est implementee dans le but
de limiter le phenomene de \emph{learning slowdown} en evitant la saturation des
neurones. Cette fonction permet au reseau de corriger ses poids plus efficacement
dans le cas d'une erreur importante. Elle ne peut etre seulement utilisee
avec une fonction d'activation \emph{sigmoid} et est construite pour annuler la
multiplication par $\sigma'(z)$ dans l'expression de $\boldsymbol\delta^{(L)}$.
\begin{equation}
	\begin{aligned}
		C(w, b) &= -\frac{1}{d^{(L)}}\sum_{j}^{d^{(L)}}{\left({\bf y}\ln({\bf\hat y}) +
		(1-{\bf y})\ln(1- {\bf \hat y})\right)}  \\
		{\bf C'}(w, b) &= \frac{1}{d^{(L)}}\cdot({\bf\hat y} - {\bf y})
	\end{aligned}
\end{equation}
NOTE: On remarque ici que les d\'eriv\'ees des deux fonctions sont identiques. En
revanche, une condition dans le code est ajout\'ee pour supprimer la multiplication
par $\sigma'(z)$.


\subsection{Eviter le sur-ajustement}\label{overfitting}
\begin{figure}[htp]
	\centering
	\includegraphics[scale=.4]{img/overfitting.png}
	\caption{erreurs sur le set d'entrainement(bleu) et de validation(rouge)}
\end{figure}
Le sur-ajustement (\emph{overfitting}) survient lorsque le reseau n'est plus
en train d'apprendre du dataset mais est en train de le m\'emoriser. Ceci se
traduit par un apprentissage trop sp\'ecifique aux donn\'ees, le reseau est
donc en train de modeliser le \emph{bruit}. Ceci survient lorsque le mod\`ele
(nombre de neurones et couches) est plus complexe que la fonction \`a
mod\'eliser. Ce sur-apprentissage a des consequences nefastes sur les
performances du reseau lorsqu'il est confronte a de nouvelles donnees il convient
donc de le detecter et de le minimiser.
\paragraph{Validation crois\'ee:} Pour detecter le sur-ajustement, il est
important de separer le set de donnees en deux une partie pour l'entrainement
l'autre pour la validation. Plusieurs methodes differentes sont disponibles
pour determiner comment separer le dataset. Plus on a de donnees dans le set
d'entrainement plus la prediction sera bonne mais moins on sera capable de
quantifier la capacit\'e de g\'en\'eralisation du r\'eseau et inversement.

Une methode interessante a citer est le \href{http://work.caltech.edu/slides/slides13.pdf}
{\emph{V-folds} ou \emph{K-folds}} qui permet de maximiser le nombre d'echantillons
d'entrainement en divisant le set complet en $V$ sous ensembles. Le reseau est
ensuite entrain\'e sur tous les sous-ensembles moins un qui est utilis\'e pour la
validation (le sous-ensemble de validation est choisi aleatoirement et change
a chaque epoque d'apprentissage). Il faut noter qu'il est important de
selectionner les echantillons de maniere aleatoire afin de ne pas biaiser
l'apprentissage du reseau.

Cependant, notre set de donnees etant deja separe en plusieurs sous-ensembles,
nous garderons ces methodes comme pistes d'amelioration futures.

\paragraph{Detection: }Le sur-ajustement peut etre aisement detecte lorsque
l'erreur sur le set de validation commence a augmenter tandis que l'erreur sur
le set d'entrainement continue de decroitre.
\paragraph{Early stopping} est une methode de limitation du sur-ajustement. Elle
consiste a monitorer les erreurs au cours de l'entrainement du reseau et
de l'interrompre sous certaines conditions(seuil, moyenne glissante, ...).

\paragraph{Les fonctions de r\'egularisation} sont une autre methode permettant
de limiter le sur-ajustement. Celles-ci ont pour approche de \emph{penaliser}
la minimisation de la fonction d'erreur dans le but d'ameliorer la capacite de
generalisation du reseau.

Ici, sont implement\'e deux des methodes les plus rependues :
r\'egularisation \emph{L1} et \emph{L2}. Toute deux visent a limiter
la magnitude poids dans l'ensemble du reseau.
Redefinissons la fonction de cout de la maniere suivante:
\begin{equation}
	\begin{aligned}
		C({\bf w}, {\bf b}) &= C_0({\bf w}, {\bf b}) + \Omega({\bf w})
	\end{aligned}
\end{equation}

O\`u $\Omega(w)$ est l'expression generale de la fonction de regularization.

On peut maintenant definir $\Omega({\bf w})$ pour chaque type de regularization:
\begin{equation}
	\begin{aligned}
		\Omega_{L1} ({\bf w}) &= \frac{\lambda}{2\cdot d^{(l)}} \cdot \sum{|{\bf w}^{(l)}|}\\
		\boldsymbol\Omega_{L1}'({\bf w}) &= \frac{\lambda}{d^{(l)}} \cdot sign\left ({\bf w}^{(l)} \right)\\
	\end{aligned}
\end{equation}
\begin{equation}
	\begin{aligned}
		\Omega_{L2} ({\bf w}) &= \frac{\lambda}{2\cdot d^{(l)}} \cdot \sum{\left({\bf w}^{(l)}\right) ^2}\\
		\boldsymbol\Omega_{L2}'({\bf w}) &= \frac{\lambda}{d^{(l)}} \cdot {\bf w}^{(l)}\\
	\end{aligned}
\end{equation}
NOTE: \href{http://work.caltech.edu/slides/slides12.pdf}{La r\'egularisation
\emph{Tikhonov}} est une forme generalis\'ee de ces approches et permet une
plus grande flexibilit\'e.

\subsection{Entrainement}
L'entrainement du reseau est un processus gourmand en calculs et demandant
beaucoup de ressources. Il est donc important de bien l'optimiser. Dans ce but,
nous sommes pass\'es par plusieurs methodes avant d'arriver a la plus optimale.

\paragraph{batch gradient descent} consiste a calculer le gradient en une seule
iteration sur l'ensemble des echantillons d'entrainement avant de pouvoir
actualiser les poids. Etant donn\'e que l'entrainement necessite un grand
nombre d'echantillons pour donner des resultats satisfaisant, cette approche
impose des calculs sur des matrices de {\bf grande} dimensions.

\paragraph{stochastic gradient descent : } contrairement \`a la methode
precedente, le gradient est ici calcule sur un seul echantillon selectionn\'e
aleatoirement (d'o\`u son nom). Ceci permet de reduire considerablement le
cout des calculs et est prouv\'e de converger vers la meme valeur que le
\emph{batch gradient descent} en montrant que l'esperance mathematique des deux
versions du gradient sont egales.

\paragraph{Mini-batch stochastic gradient descent} est la m\'ethode impl\'ement\'ee
dans le r\'eseau. Cette version est une g\'en\'eralisation des deux approches
pr\'ec\'edentes. Au lieu de selectionner un seul \'el\'ement al\'eatoirement,
le set d'entrainement est divis\'e en sous ensembles de taille $N$. Il est
interessant de noter que si $N=1$, on obtient un \emph{stochastic gradient descent}
standard et si $N$ est egal a la taille du set d'entrainement, on obtient un
\emph{batch gradient descent}.

\lstset{tabsize = 4,
frame=lines,
numbers=left,
captionpos=b,
caption = {division du set ${\bf x}$ en sous-ensembles de taille $N$},
language = python,
basicstyle=\small} \\
\begin{lstlisting}
In [1]: x = [1, 2, 3, 4, 5, 6, 7, 8, 9]
In [2]: N = 2
In [3]: print zip(*[iter(x)]*N)
[(1, 2), (3, 4), (5, 6), (7, 8)]
\end{lstlisting}

Par manque de temps, la backpropagation implement\'ee permet simplement de
calculer le gradient sur un echantillon unique (un vecteur) et non une matrice
des $N$ echantillons. Il faut donc sommer les gradients sur tous les elements de
la mini-batch: $\nabla_bC = \sum_{i}^{N}{{\nabla_bC}_i}$ (idem pour $\nabla_wC$).

Une implementation matricielle permettrait d'am\'eliorer les performances en
exploitant pleinement les capacit\'es de \emph{numpy} (le module de calcul utilis\'e).
\lstset{tabsize = 4,
frame=lines,
numbers=left,
captionpos=b,
caption = {sommation $N$ des gradients},
language = python,
basicstyle=\small} \\
\begin{lstlisting}
for x, y in mini_batch:
	# Sum all the gradients over the mini-batch
	self.feedforward(x)
	nabla_bC_i, nabla_wC_i = self.backpropagation(y)
	nabla_bC = np.add(nabla_bC, nabla_bC_i)
	nabla_wC = np.add(nabla_wC, nabla_wC_i)
\end{lstlisting}


\subsection{Ajustement des hyper-param\`etres}
% http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters

\subsection{Inspection du res\'eau}
Parmis les contraintes obligatoires, il nous est aussi demand\'e de pouvoir
inspecter les couches cach\'ees du r\'eseau. Pour ce faire, nous tirons parti du
fait que {\em Python} est un language interpr\'et\'e et qu'il fournit \`a
l'utilisateur une interface int\'eractive(voir annexe \ref{inspect}).

En combinaison avec la possibilit\'e de savegarder l'\'etat des
objects {\em Network} sous la forme de fichiers (compress\'es ou non), nous
pouvons ais\'ement analyser chaque couche (voir annexe \ref{saveload}).

D'autre part, la conception du r\'eseau permetterai, avec peu d'effort, de coupler
plusieurs objects {\em Network} en un {\em super} reseau.
Ceci pourrai \^etre fait en impl\'ementant les methodes {\em \_\_add\_\_()}
et {\em \_\_getslice\_\_()} pour respectivement concat\'ener et couper des objects
{\em Network}.

N'ayant pas eu besoin de ces op\'erations, ces m\'ethodes n'ont
cependant pas \'et\'e impl\'ement\'ees.

\section{Interface graphique}



\section{Discussions}
\subsection{Problemes rencontr\'es}
Lors de l'entrainement du reseau, nous avons d'abord ete confronte a de tres
mauvaises performances. Nous avons initialement pense que notre pr\'e-traitement
etait inefficace voir destructeur. En analysant les performances plus en detail,
nous nous sommes apercu que le reseau etait moins performant que si nous lui
faisions faire des predictions aleatoires
$\epsilon_{\bf x} \ge \left (1 - \frac{1}{9} = 88.9\% \right)$.
Ce qui est impossible et sugg\`ere que notre \'evaluation du cout \'etait fausse.

En effet, nous avions commis une erreur dans la generation de nos vecteurs de
sortie. Lorsque la prediction etait comparee a la sortie attendue, celle-ci etait
decalee de 1 (en raison des indices commencant a 0 et non a 1):
\begin{equation}
	\left (
	{\bf \hat y}_3 =
	\begin{bmatrix}
		0 \\
		0 \\
		{\color{red} 1 }\\
		0 \\
		0 \\
		0 \\
		0 \\
		0 \\
		0 \\
	\end{bmatrix}
	\right )
	\ne
	\left (
	{\bf y}_3 =
	\begin{bmatrix}
		0 \\
		0 \\
		0 \\
		{\color{red} 1 }\\
		0 \\
		0 \\
		0 \\
		0 \\
		0 \\
	\end{bmatrix}
	\right )
\end{equation}

\subsection{conclusion et pistes d'am\'elioration}
utiliser une forme matricielle pour le backpropagation

option pour utiliser une fonction d'activation different sur la couche de sortie

automatiser l'ajustement des hyper-parameters
% ajouter autres pistes d'amelioration
% conclure sur performance globale du systeme
% classification homme/femme

\appendix
\newpage
\section{sauvegarde/chargement d'une configuration}
\label{saveload}
\lstset{tabsize = 4,
frame=lines,
numbers=none,
captionpos=b,
caption = {Sauvegarde et chargement interactif d'une configuration},
language = python,
basicstyle=\small} \\
\begin{lstlisting}
In [1]: import network as n
In [2]: my_net = n.Network([3, 4, 2], activation='sigmoid',
                   cost='cross-entropy', learning_rate=0.5)
In [3]: print my_net
Neural Network      : [3, 4, 2]
Activation function : sigmoid
Cost function       : cross-entropy
Regularization func : none
learning rate       : 0.5
Regularization rate : 0.1

L0  * * *
L1 * * * *
L2   * *
In [4]: my_net.save('foo.save')
In [5]: my_net = n.Network([8, 4, 4, 2], activation='tanh',
   ...:                   cost='quadratic', learning_rate=0.02,
   ...:				      regularization='L2', lambda_ = 0.001)
In [6]: print my_net
Neural Network      : [8, 4, 4, 2]
Activation function : tanh
Cost function       : quadratic
Regularization func : L2
learning rate       : 0.02
Regularization rate : 0.001

L0 * * * * * * * *
L1     * * * *
L2     * * * *
L3       * *
In [7]: my_net.load('foo.save')
In [8]: print my_net
Neural Network      : [3, 4, 2]
Activation function : sigmoid
Cost function       : cross-entropy
Regularization func : none
learning rate       : 0.5
Regularization rate : 0.1

L0  * * *
L1 * * * *
L2   * *
\end{lstlisting}

\newpage
\section{Inspection du r\'eseau}
\label{inspect}
\lstset{tabsize = 4,
frame=lines,
numbers=none,
captionpos=b,
caption = {Exemple d'inspection interactive d'un r\'eseau de neurones},
language = python,
basicstyle=\small} \\
\begin{lstlisting}
In [1]: import network as n
In [2]: my_net = n.Network((1, 2))
In [3]: my_net.load('foo.save')                
In [4]: print my_net.struct 
[3, 4, 2]                    

In [15]: print my_net.weights[0]                                                           
[[ 0.12737432  0.97734051 -0.56505148]                                                     
 [ 0.9090921   0.19178132 -0.15200818]                                                     
 [-0.07138651 -0.59903432 -0.78958921]                                                     
 [-0.55362229  0.51319807  0.2935551 ]]                                                    
                                                                                           
In [16]: print my_net.weights[1]                                                           
[[-0.69519106  0.15539354 -0.46978019 -0.88703575]                                         
 [-0.44994898 -0.57113777 -0.3736959  -0.00245537]]                                        
                                                                                           
In [17]: cat foo.save                                                                      
# vim: set ft=yaml:                                                                        
activation: sigmoid                                                                        
biases:                                                                                    
- - [-0.920441751728251]                                                                   
  - [-0.36244765002601287]                                                                 
  - [-0.2926753984327689]                                                                  
  - [-0.12034180777419923]                                                                 
- - [1.1212748305651405]                                                                   
  - [-0.565286303186786]                                                                   
cost: cross-entropy                                                                        
eta: 0.5                                                                                   
lambda: 0.1                                                                                
regularization: none                                                                       
struct: [3, 4, 2]                                                                          
weights:                                                                                   
- - [0.12737431801447707, 0.9773405101355827, -0.5650514778232817]                         
  - [0.90909209744093, 0.19178132268834805, -0.1520081800115934]                           
  - [-0.07138650509286246, -0.5990343151487274, -0.7895892103519402]                       
  - [-0.5536222909987745, 0.5131980654305697, 0.29355509901743354]                         
- - [-0.6951910582048451, 0.1553935350379862, -0.4697801922017521, 
		-0.8870357534796379]    
  - [-0.44994897980803156, -0.5711377657299319, -0.3736959026341852,
		-0.002455368584577519]
\end{lstlisting}

\end{document}
% vim: cc=80 :
